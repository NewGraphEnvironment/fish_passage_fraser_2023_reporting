---
title: "0205_extract_inputs"
date: "Created: 2024-06-20 | Updated: `r format(Sys.Date(), '%Y-%m-%d')`"
output: 
  html_document:
    code_folding: "hide"
params:
  repo_owner: "NewGraphEnvironment"
  repo_name: "fish_passage_fraser_2023_reporting"
  gis_name: "sern_fraser_2024"
  job_name: "2024-074-sern-fraser-fish-passage"
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=TRUE, include = TRUE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = "100%", eval = FALSE)
options(scipen=999)
options(knitr.kable.NA = '--') #'--'
options(knitr.kable.NAN = '--')
```


# Purpose of this script

Add columns to `form_fiss_site_2024` and then burn back a NEW geopackage.

```{r params}
# path to raw form_fiss_site after QA
path_form_fiss_site_raw <- fs::path_expand(fs::path("~/Projects/gis/", params$gis_name, "/data_field/2024/form_fiss_site_2024_raw.gpkg"))

#NEW geopcackage path for the form after we have added columns as per this script
path_form_fiss_site <- fs::path_expand(fs::path("~/Projects/gis/", params$gis_name, "/data_field/2024/form_fiss_site_2024.gpkg"))
```

## Backup the form after the QA

```{r backup-raw}

form_fiss_site_raw <- fpr::fpr_sp_gpkg_backup(
  path_gpkg = path_form_fiss_site_raw,
  dir_backup = "data/backup/",
  update_utm = TRUE,
  update_site_id = FALSE,
  write_back_to_path = FALSE,
  return_object = TRUE,
  col_easting = "utm_easting",
  col_northing = "utm_northing"
) 
```


## Fix times

This is not always applicable and we will hopefully have a permanent fix soon.

For Fraser 2024 - times are even more funky than usual- see issue here https://github.com/NewGraphEnvironment/fish_passage_fraser_2023_reporting/issues/123

The times in `form_pscis_2024_raw` are incorrect in the form they are in UTC - but for some reason when read into R the timezone is set to America/Vancouver even though the times are in UTC.

In addition - the 4 sites I added by hand (issue here https://github.com/NewGraphEnvironment/fish_passage_fraser_2023_reporting/issues/121) are the correct time and timezone - so they don't need any fixing. 



```{r time-fix, eval F}

form_fiss_time_fix <-  form_fiss_site_raw |> 
 dplyr::mutate(date_time_start_raw = date_time_start) |> 
 dplyr::mutate(date_time_start = dplyr::case_when(time_correct == FALSE ~ lubridate::force_tz(date_time_start_raw, tzone = "UTC"), TRUE ~ date_time_start_raw),
               date_time_start = dplyr::case_when(time_correct == FALSE ~ lubridate::with_tz(date_time_start, tzone = "America/Vancouver"), TRUE ~ date_time_start_raw)) |> 
  dplyr::relocate(date_time_start_raw, .before = date_time_start)


form_fiss_site_raw <- form_fiss_time_fix |> 
  dplyr::select(-date_time_start_raw)

```

## Unique to Fraser 2024

This is unique to the Fraser 2024 project where we were surveying sites without PSCIS id but they were there they just hadn't been loaded to the project, so we need to change the local names to the pscis Ids. We need the PSCIS ids because they are used for many other things in the next scripts - for example joining watershed codes in the next section. We will keep the modelled crossing Ids in the form in case we need them later. 

We will also remove the monitoring sites because they are going to cause issues in the future I think. 

```{r update-pscis-id, eval=FALSE}

# Get the PSCIS IDs in the cross reference table in th sqlite
conn <- readwritesqlite::rws_connect("data/bcfishpass.sqlite")
xref_pscis_my_crossing_modelled <- readwritesqlite::rws_read_table("xref_pscis_my_crossing_modelled", conn = conn)
readwritesqlite::rws_disconnect(conn)


form_fiss_site_prep1 <- form_fiss_site_raw |> 
  # Fraser 2024 - remove the monitoring sites from this form.
  dplyr::filter(monitoring_form == FALSE) |> 
  # Extract the modelled crossing ID from local_name only if it does not contain "7622" or "203302" which are already PSCIS IDs
  dplyr::mutate(
    modelled_crossing_id = dplyr::case_when(
      !stringr::str_detect(local_name, "7622|203302") ~ stringr::str_extract(local_name, "^[0-9]+"),
      TRUE ~ NA_character_
    ),
    # Convert to character for joining
    modelled_crossing_id = as.character(modelled_crossing_id)
  ) |> 
  # Join with PSCIS data
  dplyr::left_join(
    xref_pscis_my_crossing_modelled |> 
      dplyr::mutate(external_crossing_reference = as.character(external_crossing_reference)),
    by = c("modelled_crossing_id" = "external_crossing_reference")
  ) |> 
  dplyr::rename(pscis_crossing_id = stream_crossing_id) |> 
  # Replace the modelled crossing ID in local_name with PSCIS ID
  dplyr::mutate(local_name = dplyr::case_when(
    !is.na(pscis_crossing_id) ~ stringr::str_replace(local_name, "^[0-9]+", as.character(pscis_crossing_id)),
    TRUE ~ local_name
  ))



form_fiss_site_raw <- form_fiss_site_prep1


```


## Clean up the form 

```{r clean-form}
# clean up the form
form_fiss_site_cleaned <- form_fiss_site_raw |>
  # split the local_name into the site, location, and ef
    tidyr::separate_wider_delim(local_name, 
                              delim = "_", 
                              names = c('site', 'location', 'ef'),
                              too_few = "align_start",
                              cols_remove = FALSE) |>  
    dplyr::mutate(
    local_name = stringr::str_trim(local_name),
    site = stringr::str_trim(site),
    location = stringr::str_trim(location),
    ef = stringr::str_trim(ef)
  ) |> 
  # split out the date and the time - change type of column first
  dplyr::mutate(date_time_start = lubridate::ymd_hms(date_time_start, tz = "America/Vancouver"),
                date_time_start = lubridate::floor_date(date_time_start, unit = "second"),  # Remove microseconds
                survey_date = lubridate::date(date_time_start),
                time = hms::as_hms(date_time_start), 
                # Fix some vocabulary. Change "trib" to long version "Tributary" etc.
                gazetted_names = stringr::str_replace_all(gazetted_names, 'Trib ', 'Tributary '),
                gazetted_names = stringr::str_to_title(gazetted_names),
                crew_members =  stringr::str_to_upper(crew_members),
                # fill in text columns from spreadsheet that will likely never change
                waterbody_type = 'stream',
                method_for_utm = 'GPS general',
                method_for_channel_width = 'metre tape',
                method_for_wetted_width = 'metre tape',
                method_for_residual_pool_depth = 'metre stick',
                method_for_bankfull_depth = 'metre stick',
                method_for_gradient = 'clinometer',
                method_for_temperature = 'recording meter',
                method_for_conductivity = 'recording meter',
                method_for_p_h = 'pH meter (general)') |> 
  # arrange by surveyor and date/time
  dplyr::arrange(mergin_user, date_time_start) |> 
  # ditch the time since we don't need anymore. Time was dropped on gpkg creation due to type conflict
  dplyr::select(-time) |>
  # rearrange the columns for easier QA in QGIS.
  dplyr::select(
    date_time_start,
    local_name,
    gazetted_names,
    crew_members,
    comments,
    everything()) |> 
  dplyr::arrange(date_time_start)

```


## Query database to get 1:50,000 watershed codes
Really kind of humorous that we are getting 1:50,000 watershed codes from the database then the province turns around
and converts them back to 1:20,000 (pers. comm. Dave McEwan - Fisheries Standards Biologist - 778 698-4010 - Dave.McEwan@gov.bc.ca).

```{r get-wsc}

ids <- form_fiss_site_cleaned |>
  dplyr::distinct(site) |> 
  dplyr::pull(site)


wscodes <- fpr::fpr_db_query(
  query = glue::glue("SELECT DISTINCT ON (stream_crossing_id)
    a.stream_crossing_id,
    a.linear_feature_id,
    a.watershed_group_code,
    b.watershed_code_50k,
    substring(b.watershed_code_50k from 1 for 3)
    || '-' || substring(b.watershed_code_50k from 4 for 6)
    || '-' || substring(b.watershed_code_50k from 10 for 5)
    || '-' || substring(b.watershed_code_50k from 15 for 5)
    || '-' || substring(b.watershed_code_50k from 20 for 4)
    || '-' || substring(b.watershed_code_50k from 24 for 4)
    || '-' || substring(b.watershed_code_50k from 28 for 3)
    || '-' || substring(b.watershed_code_50k from 31 for 3)
    || '-' || substring(b.watershed_code_50k from 34 for 3)
    || '-' || substring(b.watershed_code_50k from 37 for 3)
    || '-' || substring(b.watershed_code_50k from 40 for 3)
    || '-' || substring(b.watershed_code_50k from 43 for 3) AS watershed_code_50k_parsed,
    b.blue_line_key_20k,
    b.watershed_key_20k,
    b.blue_line_key_50k,
    b.watershed_key_50k,
    b.match_type
    FROM bcfishpass.crossings_vw a
    LEFT OUTER JOIN whse_basemapping.fwa_streams_20k_50k b
    ON a.linear_feature_id = b.linear_feature_id_20k
    WHERE a.stream_crossing_id IN ({glue::glue_collapse(glue::single_quote(ids), sep = ', ')})
    ORDER BY a.stream_crossing_id, b.match_type;"
  ) 
)

```


We need to QA the watershed codes using the `whse_fish.wdic_waterbody_route_line_svw` layer.  This layer has now been added the 2023 QGIS shared projects for Skeena and Peace. Still needs to be done for Fraser. 

FRASER 2024 - CODES STILL NEED TO BE QAED!!!!

Finally, lets join it to the form_fiss_site_raw so we can copy/paste it into the spreadsheet all at once.

```{r wsc-join}

## join watershed codes to form_fiss_site
form_fiss_site_prep2 <- dplyr::left_join(
  form_fiss_site_cleaned |> 
    dplyr::mutate(site = as.integer(site)),
  
  wscodes |> 
    dplyr::select(stream_crossing_id, watershed_code_50k = watershed_code_50k_parsed, watershed_group_code),
  
  by = c('site' = 'stream_crossing_id')) |> 
  
  dplyr::mutate(waterbody_id = paste0('00000', watershed_group_code),
                waterbody_type = 'stream')
```



## Calculate the average of the numeric columns

```{r calculate-averages}

# aggregate the numeric columns
# as per the example in ?ngr_str_df_col_agg
col_str_negate = "time|method|avg|average"
col_str_to_agg <- c("channel_width", "wetted_width", "residual_pool", "gradient", "bankfull_depth")
columns_result <- c("avg_channel_width_m", "avg_wetted_width_m", "average_residual_pool_depth_m", "average_gradient_percent", "average_bankfull_depth_m")

form_fiss_site_prep3 <- purrr::reduce(
  .x = seq_along(col_str_to_agg),
  .f = function(dat_acc, i) {
    ngr::ngr_str_df_col_agg(
      # we call the dataframe that accumulates results dat_acc
      dat = dat_acc,
      col_str_match = col_str_to_agg[i],
      col_result = columns_result[i],
      col_str_negate = col_str_negate,
      decimal_places = 1
    )
  },
  .init = form_fiss_site_prep2
)
```


## Burn back to the geopackage
Burn back to geopackage so all the new columns/data are also in the form

```{r fiss-burn-gpkg}
form_fiss_site_prep3 |> 
  sf::st_write(dsn = path_form_fiss_site,
               append = FALSE,
               delete_dsn = TRUE)

```


## Backup the new complete form 

```{r backup-complete}

fpr::fpr_sp_gpkg_backup(
  path_gpkg = path_form_fiss_site,
  dir_backup = "data/backup/",
  update_utm = TRUE,
  update_site_id = FALSE,
  write_back_to_path = FALSE,
  return_object = FALSE,
  col_easting = "utm_easting",
  col_northing = "utm_northing"
) 
```
